{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import napari\n",
    "from os.path import exists\n",
    "from brainlit.utils.session import NeuroglancerSession\n",
    "from cloudvolume.exceptions import SkeletonDecodeError\n",
    "from napari_animation import AnimationWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 9 #0,2,3,4,7,9\n",
    "janelia = False\n",
    "\n",
    "im_path = \"/Users/thomasathey/Documents/mimlab/mouselight/input/images/first10_quantitative/images/2018-08-01_\" + str(num) + \"_first10_quantitative.tif\"\n",
    "im = Path(im_path)\n",
    "img = io.imread(im, plugin=\"tifffile\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "\n",
    "# read coords\n",
    "if janelia:\n",
    "    csv_path = \"/Users/thomasathey/Documents/mimlab/mouselight/input/images/first10_quantitative/voxel_coords.csv\"\n",
    "    coords = np.genfromtxt(csv_path, delimiter=',')\n",
    "    coords = coords[10*num:10*(num+1)].astype(int)\n",
    "    soma_coords = [list(coords[0,:])]\n",
    "    axon_coords = [list(coords[-1,:])]\n",
    "else:\n",
    "    csv_path = \"/Users/thomasathey/Documents/mimlab/mouselight/input/images/first10_quantitative/my_points/points_\" + str(num) + \".csv\"\n",
    "    coords = np.genfromtxt(csv_path, delimiter=',')\n",
    "    coords = coords[1:].astype(int)\n",
    "    coords = coords[:,1:]\n",
    "    soma_coords = [list(coords[-1,:])]\n",
    "    axon_coords = [list(coords[0,:])]\n",
    "\n",
    "coords_list = list(coords)\n",
    "coords_list = [list(c) for c in coords_list]\n",
    "print(f\"coords shape: {coords.shape}\")\n",
    "\n",
    "if num == 0:\n",
    "    soma_coords += [[226, 643, 92], [233, 459, 120], [439, 500, 62]]\n",
    "elif num == 7:\n",
    "    soma_coords += [[335, 178, 39], [1, 418, 322]]\n",
    "\n",
    "print(f\"Soma coords: {soma_coords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 invalid\n",
      "#1 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 2 exists\n",
      "#3 invalid\n",
      "#4 invalid\n",
      "#5 invalid\n",
      "#6 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 7 exists\n",
      "#8 invalid\n",
      "#9 invalid\n",
      "#10 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 11 exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 12 exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 13 exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 14 exists\n",
      "#15 invalid\n",
      "#16 invalid\n",
      "#17 invalid\n",
      "#18 invalid\n",
      "#19 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 20 exists\n",
      "#21 invalid\n",
      "#22 invalid\n",
      "#23 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 24 exists\n",
      "#25 invalid\n",
      "#26 invalid\n",
      "#27 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 28 exists\n",
      "#29 invalid\n",
      "#30 invalid\n",
      "#31 invalid\n",
      "#32 invalid\n",
      "#33 invalid\n",
      "#34 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 35 exists\n",
      "#36 invalid\n",
      "#37 invalid\n",
      "#38 invalid\n",
      "#39 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:03<00:00,  3.76s/it]\n",
      "Downloading: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 40 exists\n",
      "#41 invalid\n",
      "#42 invalid\n",
      "#43 invalid\n",
      "#44 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 45 exists\n",
      "#46 invalid\n",
      "#47 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 48 exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 49 exists\n",
      "#50 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 51 exists\n",
      "#52 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 53 exists\n",
      "#54 invalid\n",
      "#55 invalid\n",
      "#56 invalid\n",
      "#57 invalid\n",
      "#58 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 59 exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 60 exists\n",
      "#61 invalid\n",
      "#62 invalid\n",
      "#63 invalid\n",
      "#64 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 65 exists\n",
      "#66 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 67 exists\n",
      "#68 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 69 exists\n",
      "#70 invalid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 71 exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton # 72 exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (326, 239, 74)\n"
     ]
    }
   ],
   "source": [
    "dir = \"s3://open-neurodata/brainlit/brain1\"\n",
    "dir_segments = \"s3://open-neurodata/brainlit/brain1_segments\"\n",
    "\n",
    "ngl_sess = NeuroglancerSession(mip = 0, url = dir, url_segments=dir_segments)\n",
    "\n",
    "num_goal = 22\n",
    "num = -1\n",
    "skel_id = 0\n",
    "\n",
    "while num < num_goal:\n",
    "    try:\n",
    "        ngl_sess.pull_vertex_list(skel_id, [0], 1)\n",
    "        print(f\"Skeleton # {skel_id} exists\")\n",
    "        num += 1\n",
    "    except SkeletonDecodeError:\n",
    "        print(f\"#{skel_id} invalid\")\n",
    "    skel_id += 1\n",
    "\n",
    "img, bbox, vox = ngl_sess.pull_vertex_list(skel_id-1, [0,1,2,3,4,5,6,7,8,9], [50,50,15])\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "coords_list = vox\n",
    "soma_coords = [coords_list[0]]\n",
    "axon_coords = [coords_list[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read APP\n",
    "app_reflect_y = False\n",
    "\n",
    "app_path, _ = read_swc(\"/Users/thomasathey/Documents/mimlab/mouselight/input/images/firstn_quantitative/tif/\" + str(num) + \".swc\",\n",
    "                    axon_coords[0], soma_coords[0], reflect_y=app_reflect_y)\n",
    "new_app = np.multiply(np.array(app_path), [1,1,1])\n",
    "new_app = resample(new_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(img)\n",
    "viewer.add_shapes(coords_list, shape_type='path', edge_width=2, edge_color='green', name=\"truth\")\n",
    "#viewer.add_shapes(new_app, shape_type='path', edge_width=2, edge_color='red', name=\"truth\")\n",
    "\n",
    "viewer.add_points(soma_coords, face_color='orange', size=8)\n",
    "viewer.add_points(axon_coords, face_color='red', size=8)\n",
    "viewer.camera.angles = [0, -90, -90]\n",
    "napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_path = \"/Users/thomasathey/Documents/mimlab/mouselight/input/images/firstn_quantitative/zarr/\" + str(num) + \".zarr\"\n",
    "\n",
    "z = zarr.zeros(img.shape, chunks=img.shape, dtype=img.dtype)\n",
    "z = img\n",
    "zarr.save(zarr_path, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainlit.algorithms.generate_fragments.state_generation import state_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = state_generation(image_path=\"/Users/thomasathey/Documents/mimlab/mouselight/input/images/firstn_quantitative/zarr/\" + str(num) + \".zarr\",\n",
    "        ilastik_program_path=\"/Applications/ilastik-1.3.3post3-OSX.app/Contents/ilastik-release/run_ilastik.sh\",\n",
    "        ilastik_project_path=\"/Users/thomasathey/Documents/mimlab/mouselight/octopus_experiment/octopus_exp.ilp\",\n",
    "        chunk_size=img.shape,\n",
    "        soma_coords=soma_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.predict(data_bin=\"/Users/thomasathey/Documents/mimlab/mouselight/input/images/first10_quantitative/zarr/misc/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.compute_frags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.compute_soma_lbls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.compute_image_tiered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.compute_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.compute_edge_weights()\n",
    "viterbrain = sg.viterbrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/Users/thomasathey/Documents/mimlab/mouselight/input/images/firstn_quantitative/zarr/' + str(num) + '_viterbrain.pickle', 'rb') as handle:\n",
    "    viterbrain = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb_path = viterbrain.shortest_path(axon_coords[0], soma_coords[0])\n",
    "vb_path = [list(coord) for coord in vb_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(img)\n",
    "viewer.add_shapes(vb_path, shape_type='path', edge_width=2, edge_color='red', name=\"viterbrain\")\n",
    "viewer.add_shapes(coords_list, shape_type='path', edge_width=2, edge_color='green', name=\"truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load other paths and resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from cloudvolume import Skeleton\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from brainlit.viz import Bresenham3D\n",
    "import similaritymeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_swc(file_path, start_pt, end_pt, switch_axes = True, reflect_y = True, factor = [1,1,1]):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        swc = f.read()\n",
    "    skel = Skeleton.from_swc(swc)\n",
    "\n",
    "\n",
    "    verts = np.array(skel.vertices)\n",
    "    if switch_axes:\n",
    "        verts[:, [0,2]] = verts[:, [2,0]]\n",
    "    if reflect_y:\n",
    "        verts[:,1] = im_og.shape[1] - 1 - verts[:,1]\n",
    "    verts = np.multiply(verts, factor)\n",
    "    skel.vertices = verts\n",
    "\n",
    "    # find path\n",
    "    amins, _ = pairwise_distances_argmin_min(np.array([end_pt]), skel.vertices)\n",
    "    soma_id = amins[0]\n",
    "\n",
    "    amins, _ = pairwise_distances_argmin_min(np.array([start_pt]), skel.vertices)\n",
    "    axon_id = amins[0]\n",
    "\n",
    "    g = nx.Graph()\n",
    "    num_verts = skel.vertices.shape[0]\n",
    "    g.add_nodes_from(np.arange(num_verts))\n",
    "    for e in tqdm(skel.edges):\n",
    "        g.add_edge(e[0],e[1])\n",
    "        \n",
    "    try:\n",
    "        graph_path = nx.shortest_path(g, source=axon_id, target=soma_id)\n",
    "    except:\n",
    "        graph_path = [axon_id]\n",
    "\n",
    "    path = []\n",
    "    for id in graph_path:\n",
    "        path.append(skel.vertices[id, :])\n",
    "    path = np.array(path)\n",
    "    path = np.concatenate(([start_pt], path, [end_pt]), axis=0)\n",
    "\n",
    "    #make mask\n",
    "    skel_mask = 0*img\n",
    "\n",
    "    # for edge in tqdm(skel.edges, desc='Drawing edges...'):\n",
    "    #     pt1 = skel.vertices[edge[0],:].astype(int)\n",
    "    #     pt2 = skel.vertices[edge[1],:].astype(int)\n",
    "    #     xs, ys, zs = Bresenham3D(int(pt1[0]), int(pt1[1]), int(pt1[2]),int(pt2[0]), int(pt2[1]), int(pt2[2]))\n",
    "    #     skel_mask[xs, ys, zs] = 1\n",
    "\n",
    "    return path, skel_mask\n",
    "\n",
    "def resample(path, spacing = 1):\n",
    "    new_path = []\n",
    "    for n in np.arange(path.shape[0]):\n",
    "        pt1 = path[n-1:n,:]\n",
    "        pt2 = path[n:n+1,:]\n",
    "\n",
    "        new_path.append(pt1)\n",
    "        dist = np.linalg.norm(pt1-pt2)\n",
    "\n",
    "        if dist > 1:\n",
    "            ts = np.arange(0, dist , 5)\n",
    "            mid = np.zeros((len(ts)-1,3))\n",
    "            for i,t in enumerate(ts[1:]):\n",
    "                mid[i,:] = pt1 + (t/dist)*(pt2 -  pt1)\n",
    "            new_path.append(mid)\n",
    "    new_path.append(pt2)\n",
    "    new_path = np.concatenate(new_path)\n",
    "    return new_path\n",
    "\n",
    "def sd(pts1, pts2, substantial = False, verbose=False):\n",
    "    _, dists1 = pairwise_distances_argmin_min(pts1, pts2)\n",
    "    _, dists2 = pairwise_distances_argmin_min(pts2, pts1)\n",
    "    if verbose:\n",
    "        print(dists1)\n",
    "        print(dists2)\n",
    "    if substantial:\n",
    "        ddiv1 = np.mean(dists1[dists1 > 2])\n",
    "        ddiv2 = np.mean(dists2[dists2 > 2])\n",
    "        return np.mean([ddiv1, ddiv2])\n",
    "    else:\n",
    "        ddiv1 = np.mean(dists1)\n",
    "        ddiv2 = np.mean(dists2)\n",
    "        return np.mean([ddiv1, ddiv2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_true = np.multiply(np.array(coords_list), [0.3,0.3,1])\n",
    "new_true_resample = resample(new_true)\n",
    "new_true_resample = np.flip(new_true_resample, axis=0)\n",
    "new_viterbi = np.multiply(np.array(vb_path), [0.3,0.3,1])\n",
    "new_viterbi = resample(new_viterbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:00<00:00, 220824.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# app_reflect_y = False\n",
    "\n",
    "# ad_path, _ = read_swc(\"/Users/thomasathey/Documents/mimlab/mouselight/input/images/firstn_quantitative/advantra/\" + str(num) + \".tif_Advantra.swc\",\n",
    "#                     axon_coords[0], soma_coords[0], reflect_y=app_reflect_y)\n",
    "# new_ad = np.multiply(np.array(ad_path), [0.3,0.3,1])\n",
    "# new_ad = resample(new_ad)\n",
    "\n",
    "app_reflect_y = False\n",
    "\n",
    "app_path, _ = read_swc(\"/Users/thomasathey/Documents/mimlab/mouselight/input/images/firstn_quantitative/app2/\" + str(num) + \".swc\",\n",
    "                    axon_coords[0], soma_coords[0], reflect_y=app_reflect_y)\n",
    "new_app = np.multiply(np.array(app_path), [0.3,0.3,1])\n",
    "new_app = resample(new_app)\n",
    "\n",
    "# #gtree\n",
    "# switch_axes = True\n",
    "# factor = [1/0.3,1/0.3,1]\n",
    "# reflect_y = False\n",
    "\n",
    "# gtree_path, gtree_mask = read_swc(\"/Users/thomasathey/Documents/mimlab/mouselight/input/images/firstn_quantitative/other_traces_2/gtree_\" + str(num) + \"/gtree_\" + str(num) + \"_000.swc\",\n",
    "#                     axon_coords[0], soma_coords[0], switch_axes=switch_axes, reflect_y=reflect_y, factor=factor)\n",
    "# new_gtree = np.multiply(np.array(gtree_path), [0.3,0.3,1])\n",
    "# new_gtree = resample(new_gtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1/1 [00:19<00:00, 19.60s/it]\n",
      "Downloading: 100%|██████████| 27/27 [00:17<00:00,  1.53it/s]\n",
      "/Users/thomasathey/Documents/mimlab/mouselight/docs_env/lib/python3.8/site-packages/napari_animation/_qt/keyframeslist_widget.py:156: FutureWarning: Themes were changed to use evented model with Pydantic's color type rather than the `rgb(x, y, z)`. You can get the old color by calling `color.as_rgb()`. The `as_dict=True` option will be removed in 0.X.X\n",
      "  self.setStyleSheet(template(qss_template, **get_theme(theme_name)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<napari._qt.widgets.qt_viewer_dock_widget.QtViewerDockWidget at 0x1e0972790>"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(img, scale=[0.3,0.3,1])\n",
    "viewer.add_shapes(new_viterbi, shape_type='path', edge_width=2, edge_color='red', name=\"viterbrain\")\n",
    "viewer.add_shapes(new_true_resample, shape_type='path', edge_width=2, edge_color='green', name=\"truth\")\n",
    "viewer.add_shapes(new_app, shape_type='path', edge_width=2, edge_color='blue', name=\"app2\")\n",
    "#viewer.add_shapes(new_ad, shape_type='path', edge_width=2, edge_color='blue', name=\"advantra\")\n",
    "#viewer.add_shapes(new_gtree, shape_type='path', edge_width=2, edge_color='orange', name=\"gtree\")\n",
    "animation_widget = AnimationWidget(viewer)\n",
    "viewer.window.add_dock_widget(animation_widget, area=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi: Frechet: 9.256349172324907, SSD: 2.209227533490179\n",
      "APP2: Frechet: 3.110713538835817, SSD: 1.5373972976606955\n",
      "Advantra: Frechet: 185.8722410689665, SSD: 1.5373972976606955\n"
     ]
    }
   ],
   "source": [
    "print(f\"Viterbi: Frechet: {similaritymeasures.frechet_dist(new_true_resample, new_viterbi)}, SSD: {sd(new_true_resample, new_viterbi, substantial = False)}\")\n",
    "print(f\"APP2: Frechet: {similaritymeasures.frechet_dist(new_true_resample, new_app)}, SSD: {sd(new_true_resample, new_app, substantial = False)}\")\n",
    "print(f\"Advantra: Frechet: {similaritymeasures.frechet_dist(new_true_resample, new_ad)}, SSD: {sd(new_true_resample, new_app, substantial = False)}\")\n",
    "# print(f\"gtree: Frechet: {similaritymeasures.frechet_dist(new_true_resample, new_gtree)}, SSD: {sd(new_true_resample, new_gtree, substantial = False)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read my points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/Users/thomasathey/Documents/mimlab/mouselight/input/images/firstn_quantitative/manual/points_\" + str(num) + \".csv\"\n",
    "coords = np.genfromtxt(csv_path, delimiter=',')\n",
    "coords = coords[1:].astype(int)\n",
    "coords = coords[:,1:]\n",
    "soma_coords = [list(coords[-1,:])]\n",
    "axon_coords = [list(coords[0,:])]\n",
    "\n",
    "coords_list = list(coords)\n",
    "coords_list = [list(c) for c in coords_list]\n",
    "\n",
    "vb_path = viterbrain.shortest_path(axon_coords[0], soma_coords[0])\n",
    "vb_path = [list(coord) for coord in vb_path]\n",
    "\n",
    "new_true = np.multiply(np.array(coords_list), [0.3,0.3,1])\n",
    "new_true_resample = resample(new_true)\n",
    "new_viterbi = np.multiply(np.array(vb_path), [0.3,0.3,1])\n",
    "new_viterbi = resample(new_viterbi)\n",
    "\n",
    "app_reflect_y = False\n",
    "\n",
    "ad_path, _ = read_swc(\"/Users/thomasathey/Documents/mimlab/mouselight/input/images/firstn_quantitative/advantra/\" + str(num) + \".tif_Advantra.swc\",\n",
    "                    axon_coords[0], soma_coords[0], reflect_y=app_reflect_y)\n",
    "new_ad = np.multiply(np.array(ad_path), [0.3,0.3,1])\n",
    "new_ad = resample(new_ad)\n",
    "\n",
    "app_reflect_y = False\n",
    "\n",
    "app_path, _ = read_swc(\"/Users/thomasathey/Documents/mimlab/mouselight/input/images/firstn_quantitative/app2/\" + str(num) + \".swc\",\n",
    "                    axon_coords[0], soma_coords[0], reflect_y=app_reflect_y)\n",
    "new_app = np.multiply(np.array(app_path), [0.3,0.3,1])\n",
    "new_app = resample(new_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Viterbi: Frechet: {similaritymeasures.frechet_dist(new_true_resample, new_viterbi)}, SSD: {sd(new_true_resample, new_viterbi, substantial = False)}\")\n",
    "print(f\"APP2: Frechet: {similaritymeasures.frechet_dist(new_true_resample, new_app)}, SSD: {sd(new_true_resample, new_app, substantial = False)}\")\n",
    "print(f\"Advantra: Frechet: {similaritymeasures.frechet_dist(new_true_resample, new_ad)}, SSD: {sd(new_true_resample, new_app, substantial = False)}\")\n",
    "# print(f\"gtree: Frechet: {similaritymeasures.frechet_dist(new_true_resample, new_gtree)}, SSD: {sd(new_true_resample, new_gtree, substantial = False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5dc00d68ff54f8375e99934614da4863299fb9e10af4294c095b7f517546ff26"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('docs_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

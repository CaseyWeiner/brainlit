{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading neurons from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "%gui qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainlit\n",
    "from brainlit.utils.ngl_pipeline import NeuroglancerSession\n",
    "from brainlit.viz.swc import *\n",
    "import numpy as np\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading entire neuron from AWS \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`napari.components.viewer_model.ViewerModel.add_swc` does this via the following functions in the `napari.layers.swc.swc` module\n",
    "1. `swc.read_s3` to read the s3 file into a pd.DataFrame\n",
    "2. `swc.read_swc` to read the swc file into a pd.DataFrame\n",
    "3. `swc.generate_df_subset` creates a smaller subset of the original dataframe with coordinates in img space\n",
    "4. `swc.swc_to_voxel` to convert the coordinates from spatial to voxel coordinates\n",
    "5. `swc.df_to_graph` to convert the DataFrame into a netwrokx.DiGraph\n",
    "6. `swc.graph_to_paths` to convert from a graph into a list of paths\n",
    "7. `ViewerModel.add_shapes` to add the paths as a shape layer into the napari viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `read_s3`\n",
    "This function parses the swc file into a pd.DataFrame. Each row is a vertex in the swc file with the following information: \n",
    "\n",
    "`sample number`\n",
    "\n",
    "`structure identifier`\n",
    "\n",
    "`x coordinate`\n",
    "\n",
    "`y coordinate`\n",
    "\n",
    "`z coordinate`\n",
    "\n",
    "`radius of dendrite`\n",
    "\n",
    "`sample number of parent`\n",
    "\n",
    "The coordinates are given in spatial units of micrometers ([swc specification](http://www.neuronland.org/NLMorphologyConverter/MorphologyFormats/SWC/Spec.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = \"s3://open-neurodata/brainlit/brain1_segments\"\n",
    "seg_id = 2\n",
    "mip = 1\n",
    "df = read_s3(s3_path, seg_id, mip)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `swc.read_swc`\n",
    "This function parses the swc file into a pd.DataFrame. Each row is a vertex in the swc file with the following information: \n",
    "\n",
    "`sample number`\n",
    "\n",
    "`structure identifier`\n",
    "\n",
    "`x coordinate`\n",
    "\n",
    "`y coordinate`\n",
    "\n",
    "`z coordinate`\n",
    "\n",
    "`radius of dendrite`\n",
    "\n",
    "`sample number of parent`\n",
    "\n",
    "The coordinates are given in spatial units of micrometers ([swc specification](http://www.neuronland.org/NLMorphologyConverter/MorphologyFormats/SWC/Spec.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consen_neuron_path = '2018-08-01_G-002_consensus.swc'\n",
    "\n",
    "df = read_swc(swc_path=consen_neuron_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `generate_df_subset`\n",
    "This function parses the swc file into a pd.DataFrame. Each row is a vertex in the swc file with the following information: \n",
    "\n",
    "`sample number`\n",
    "\n",
    "`structure identifier`\n",
    "\n",
    "`x coordinate`\n",
    "\n",
    "`y coordinate`\n",
    "\n",
    "`z coordinate`\n",
    "\n",
    "`radius of dendrite`\n",
    "\n",
    "`sample number of parent`\n",
    "\n",
    "The coordinates are given in same spatial units as the image file when using `ngl.pull_vertex_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose vertices to use for the subneuron\n",
    "subneuron_df = df[0:3] \n",
    "vertex_list = subneuron_df['sample'].array \n",
    "\n",
    "# Define a neuroglancer session\n",
    "url = \"s3://open-neurodata/brainlit/brain1\"\n",
    "mip = 1\n",
    "ngl = NeuroglancerSession(url, mip=mip)\n",
    "\n",
    "# Get vertices\n",
    "seg_id = 2\n",
    "buffer = [10, 10, 10]\n",
    "img, bounds, vox_in_img_list = ngl.pull_vertex_list(seg_id, vertex_list, buffer = buffer, expand = True)\n",
    "\n",
    "df_subneuron = generate_df_subset(subneuron_df,vox_in_img_list)\n",
    "print(df_subneuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `swc_to_voxel`\n",
    "\n",
    "If we want to overlay the swc file with a corresponding image, we need to make sure that they are in the same coordinate space. Because an image in an array of voxels, it makes sense to convert the vertices in the dataframe from spatial units into voxel units.\n",
    "\n",
    "Given the `spacing` (spatial units/voxel) and `origin` (spatial units) of the image, `swc_to_voxel` does the conversion by using the following equation:\n",
    "\n",
    "$voxel = \\frac{spatial - origin}{spacing}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = np.array([0.29875923,0.3044159,0.98840415])\n",
    "origin = np.array([70093.276,15071.596,29306.737])\n",
    "\n",
    "df_voxel = swc_to_voxel(df=df, spacing=spacing, origin=origin)\n",
    "df_voxel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. `df_to_graph`\n",
    "A neuron is a graph with no cycles (tree). While napari does not support displaying graph objects, it can display multiple paths. \n",
    "\n",
    "The DataFrame already contains all the possible edges in the neurons. Each row in the DataFrame is an edge. For example, from the above we can see that `sample 2` has `parent 1`, which represents edge `(1,2)`. `sample 1` having `parent -1` means that `sample 1` is the root of the tree.\n",
    "\n",
    "`swc.df_to_graph` reads DataFrame and converts it into a networkx directional graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = df_to_graph(df)\n",
    "print('Number of nodes:', len(G.nodes))\n",
    "print('Number of edges:', len(G.edges))\n",
    "print('\\n')\n",
    "print('Sample 1 coordinates (x,y,z)')\n",
    "print(G.nodes[1]['x'],G.nodes[1]['y'],G.nodes[1]['z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. `graph_to_paths`\n",
    "This function takes in a graph and returns a list of non-overlapping paths. The union of the paths forms the graph.\n",
    "\n",
    "The algorithm works by:\n",
    "\n",
    "1. Find longest path in the graph ([networkx.algorithms.dag.dag_longest_path](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.dag.dag_longest_path.html))\n",
    "2. Remove longest path from graph\n",
    "3. Repeat steps 1 and 2 until there are no more edges left in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = graph_to_paths(G=G)\n",
    "print(f\"The graph was decomposed into {len(paths)} paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. `ViewerModel.add_shapes`\n",
    "napari displays \"layers\". The most common layer is the image layer. In order to display the neuron, we use `path` from the [shapes](https://napari.org/tutorials/shapes) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_shapes(data=paths, shape_type='path', edge_color='white', name='Skeleton 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading sub-neuron\n",
    "\n",
    "The image of the entire brain has dimensions of (33792, 25600, 13312) voxels. G-002 spans a sub-image of (7386, 9932, 5383) voxels. Both are too big to load in napari and overlay the neuron.\n",
    "To circumvent this, we can crop out a smaller region of the neuron, load the sub-neuron, and load the corresponding sub-image.\n",
    "\n",
    "In order to get a sub-neuron, we need to specify the `bounding_box` that will be used to crop the neuron. `bounding_box` is a length 2 tuple. The first element is one corner of the bounding box (inclusive) and the second element is the opposite corner of the bounding box (exclusive). Both corners are in voxel units.\n",
    "\n",
    "`add_swc` can do all of this automatically when given `bounding_box` by following these steps:\n",
    "\n",
    "1. `read_s3` to read the swc file into a pd.DataFrame\n",
    "2. `swc_to_voxel` to convert the coordinates from spatial to voxel coordinates\n",
    "3. `df_to_graph` to convert the DataFrame into a netwrokx.DiGraph\n",
    "**3.1 `swc.get_sub_neuron` to crop the graph by `bounding_box`**\n",
    "4. `graph_to_paths` to convert from a graph into a list of paths\n",
    "5. `ViewerModel.add_shapes` to add the paths as a shape layer into the napari viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. `get_sub_neuron`\n",
    "This function crops a graph by removing edges. It removes edges that do not intersect the bounding box.\n",
    "\n",
    "Edges that intersect the bounding box will have at least one of its vertices be contained by the bounding box. The algorithm follows this principle by checking the neighborhood of vertices.\n",
    "\n",
    "For each vertex *v* in the graph:\n",
    "\n",
    "1. Find vertices belonging to local neighborhood of *v*\n",
    "2. If vertex *v* or any of its local neighborhood vertices are in the bounding box, do nothing. Otherwise, remove vertex *v* and its edges from the graph\n",
    "\n",
    "We check the neighborhood of *v* along with *v* because we want the sub-neuron to show all edges that pass through the bounding box, including edges that are only partially contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an NGL session to get the bounding box\n",
    "ngl_sess = NeuroglancerSession(mip = 1)\n",
    "img, bbbox, vox = ngl_sess.pull_chunk(2, 300, 1, 1, 1)\n",
    "bbox = bbbox.to_list()\n",
    "box = (bbox[:3], bbox[3:])\n",
    "print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sub = get_sub_neuron(G, box)\n",
    "paths_sub = graph_to_paths(G_sub)\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_shapes(data=paths_sub, shape_type='path', edge_color='blue', name='sub-neuron')\n",
    "\n",
    "# overlay corresponding image\n",
    "image_path = 'G-002_15312-4400-6448_15840-4800-6656.tif'\n",
    "img_comp = io.imread(image_path)\n",
    "img_comp = np.swapaxes(img_comp,0,2)\n",
    "\n",
    "viewer.add_image(img_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
